{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38410b37-0030-43b2-bd80-e17ae64edfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the 'requests' library to handle HTTP requests\n",
    "import requests\n",
    "\n",
    "# Importing 'BeautifulSoup' from the 'bs4' library to parse HTML and XML documents\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b6fcf8-411f-4f89-8d33-d21d62b8e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the BeautifulSoup object which contains the web structure and content of a URL\n",
    "def web_extract(url):\n",
    "    # Declare variable soup and initialize it to None\n",
    "    soup = None\n",
    "    try:\n",
    "        # Make an HTTP request to the given URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the HTTP request is successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the response using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            print(\"Website successfully retrieved\")\n",
    "        # If there is any issue with the HTTP request, print the status code\n",
    "        else:\n",
    "            print(f\"Fail to retrieve the web page. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that occur during the HTTP request\n",
    "        print(f\"Error processing the link {url}: {e}\")\n",
    "    \n",
    "    # Return the BeautifulSoup object\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6921dd44-eb64-4b0b-9b42-294273c80cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the URL of the base page of \n",
    "#->Innovate UK's Investor Partnerships Future Economy \n",
    "#-->projects to the variable baseURL\n",
    "baseURL = 'https://iuk.ktn-uk.org/projects/?_sft_programme=investor-partnerships-future-economy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7713f382-8159-4061-ade2-d72508d2d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class attribute to help extract each of the investors description page link\n",
    "targetClass =  ['group', 'border-2', 'border-primary', 'flex', 'flex-col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51900587-a4c4-47d1-9d1d-4a595929bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDescriptionPageLink(soup, targetClass):\n",
    "    \"\"\"\n",
    "    Extracts links to description pages from a BeautifulSoup object.\n",
    "    \n",
    "    Parameters:\n",
    "    soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML of the webpage.\n",
    "    targetClass (list): The class attribute to filter the target <a> tags.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of links to the description pages.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all <a> tags with the specified class attribute\n",
    "    target_a_tag = [tag for tag in soup.find_all('a')\n",
    "                    if 'class' in tag.attrs\n",
    "                    and tag.attrs['class'] == targetClass]\n",
    "\n",
    "    # Extract the href attribute (link) from each <a> tag\n",
    "    hrefList = [tag.get('href') for tag in target_a_tag]\n",
    "\n",
    "    return hrefList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44348962-598b-41b7-b439-76dd478f227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextPage(soup):\n",
    "    \"\"\"\n",
    "    Retrieves the URL of the next page from the BeautifulSoup object.\n",
    "\n",
    "    Parameters:\n",
    "    soup (BeautifulSoup): BeautifulSoup object representing the current page.\n",
    "\n",
    "    Returns:\n",
    "    str or None: URL of the next page if found, otherwise None.\n",
    "    \"\"\"\n",
    "    next_atag = soup.find(lambda tag: tag.name == 'a' and tag.text == 'Next >')\n",
    "    \n",
    "    if next_atag:\n",
    "        return next_atag.get('href')\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed90601b-129a-4b71-b880-3cd218eba2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_investorsLinkDescPagRecursively(URL, pageNum, target_class_, invList):\n",
    "    \"\"\"\n",
    "    Recursively navigates through pages to collect links to investor description pages.\n",
    "    \n",
    "    Parameters:\n",
    "    URL (str): The URL of the current page.\n",
    "    pageNum (int): The current page number.\n",
    "    target_class_ (list): The class attribute to filter the target <a> tags.\n",
    "    invList (list): The list to store the collected links.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of all collected links to investor description pages from all pages.\n",
    "    \"\"\"\n",
    "    # Create a copy of the current list of links\n",
    "    currentList = invList.copy()\n",
    "    \n",
    "    # Print the current page number\n",
    "    print(f\"Page {pageNum}: \", end='')\n",
    "    \n",
    "    # Extract the BeautifulSoup object from the URL\n",
    "    soup = web_extract(URL)\n",
    "    \n",
    "    # Extend the current list with links extracted from the current page\n",
    "    currentList.extend(getDescriptionPageLink(soup, target_class_))\n",
    "    \n",
    "    # Get the URL for the next page\n",
    "    nextURL = getNextPage(soup)\n",
    "    \n",
    "    # If there is a next page, increment the page number and continue recursion\n",
    "    if nextURL:\n",
    "        pageNum += 1\n",
    "        return get_all_investorsLinkDescPagRecursively(nextURL, pageNum, target_class_, currentList)\n",
    "    else:\n",
    "        # Return the compiled list of links when no more pages are available\n",
    "        return currentList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c2a0c8-17d6-43f1-833f-2328cf60fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInvestorName(in_soup):\n",
    "    \"\"\"\n",
    "    Extracts the investor's name from the title of the investor's description page.\n",
    "\n",
    "    Parameters:\n",
    "    in_soup (BeautifulSoup object): The BeautifulSoup object representing the investor's \n",
    "    description page.\n",
    "\n",
    "    Returns:\n",
    "    str: The extracted investor's name.\n",
    "    \"\"\"\n",
    "    # Extract the text from the <title> tag of the investor's description page\n",
    "    name = in_soup.title.text\n",
    "    \n",
    "    # Split the title text by the '-' character to separate the name from any additional details\n",
    "    splitName = name.split('-')\n",
    "    \n",
    "    # Take the first part of the split text as the investor's name and remove any leading/trailing whitespace\n",
    "    name = splitName[0].strip()\n",
    "    \n",
    "    # Return the cleaned investor's name\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "345ff8e7-6b5e-4808-b655-c1da70c8e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInvestorSpecificPage(in_soup):\n",
    "    \"\"\"\n",
    "    Extracts the URL of the specific investor's page from the BeautifulSoup object of the \n",
    "    investor's description page.\n",
    "\n",
    "    Parameters:\n",
    "    in_soup (BeautifulSoup object): The BeautifulSoup object representing the investor's \n",
    "    description page.\n",
    "\n",
    "    Returns:\n",
    "    str or None: The URL of the specific investor's page if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # Find the tag with the specified class attributes indicating the link to the investor's page\n",
    "    targetTag = in_soup.find(lambda tag: 'class' in tag.attrs and \n",
    "                          tag.attrs['class'] == ['line-clamp-1', 'underline'])\n",
    "    \n",
    "    # If the target tag is found, extract the href attribute from the <a> tag within it\n",
    "    if targetTag:\n",
    "       return targetTag.find('a').get('href')\n",
    "    else:\n",
    "        # Return None if the target tag is not found\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69aa8fe7-e73e-4ff6-81e2-d909ae5bf155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInvestorPartnersEmail(in_soup):\n",
    "    \"\"\"\n",
    "    Extracts the email address of the investor's partner from the BeautifulSoup \n",
    "    object of the investor's description page.\n",
    "\n",
    "    Parameters:\n",
    "    in_soup (BeautifulSoup object): The BeautifulSoup object representing the \n",
    "    investor's description page.\n",
    "\n",
    "    Returns:\n",
    "    str or None: The email address of the investor's partner if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # Find the <a> tag with an 'href' attribute starting with 'mailto:'\n",
    "    tempTag = in_soup.find('a', href=lambda href: href and href.startswith('mailto:'))\n",
    "    \n",
    "    # If the <a> tag is found, extract and return the email address\n",
    "    if tempTag:\n",
    "        mailTo = tempTag.get('href')  # Get the 'href' attribute value\n",
    "        splitMailText = mailTo.split(':')  # Split the string at ':'\n",
    "        return splitMailText[1]  # Return the part after ':'\n",
    "    else:\n",
    "        # Return None if no email address is found\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8adc3628-b6f6-4999-90de-462f66bd64ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d18acc72-0165-4cd5-840d-b6c1f666d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPrompt(in_text):\n",
    "    \"\"\"\n",
    "    Creates a prompt for extracting specific information from a given text.\n",
    "\n",
    "    Parameters:\n",
    "    in_text (str): The input text from which to extract information.\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted prompt string for extracting \"Investment Amount\" and \"Sector of focus\".\n",
    "    \"\"\"\n",
    "    # Format the prompt with instructions for extraction and include the input text\n",
    "    prompt = f\"\"\"\n",
    "    Extract \"Investment Amount\" and \"Sector of focus\" from this text. Return the output in two lines of text in this format:\n",
    "    \"Investment amount\": 'investment amount' if provided in the text OR 'None' if not provided\n",
    "    \"Sector of focus\": 'sector of focus' if provided in the text OR 'None' if not provided:\n",
    "    {in_text}\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "689f0064-8bb9-4109-a27c-25192fcc04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptGPT_API(prompt):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the GPT API and streams the response.\n",
    "\n",
    "    Parameters:\n",
    "    prompt (str): The prompt to send to the GPT model.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing the response chunks from the GPT model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the OpenAI client (assuming the API key is set in the environment)\n",
    "    client = OpenAI()\n",
    "\n",
    "    #You can assign the API key directly in the script by using:\n",
    "    #client = OpenAI(api_key='your-api-key')\n",
    "\n",
    "    # Create a streaming chat completion request with the specified model and prompt\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    # Initialize a list to store the response chunks\n",
    "    response = []\n",
    "\n",
    "    # Iterate over the streamed response chunks\n",
    "    for chunk in stream:\n",
    "        # Check if the current chunk has content\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            # Extract the content from the current chunk\n",
    "            content = chunk.choices[0].delta.content\n",
    "            # Append the content to the response list\n",
    "            response.append(content)\n",
    "            # Print the content to the console without adding a newline\n",
    "            #print(chunk.choices[0].delta.content, end=\"\")\n",
    "    \n",
    "    # Print a newline character to end the streaming output\n",
    "    #print()\n",
    "    \n",
    "    # Return the list of response chunks\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "669905a0-f6f0-4d03-9087-b1ebc8e25fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanTextFromSoup(soup_obj):\n",
    "    \"\"\"\n",
    "    Cleans and extracts text from a BeautifulSoup object by removing newline,\n",
    "    tab, and carriage return characters.\n",
    "\n",
    "    Parameters:\n",
    "    soup_obj (BeautifulSoup): The BeautifulSoup object from which to extract and clean text.\n",
    "\n",
    "    Returns:\n",
    "    str: The cleaned text extracted from the BeautifulSoup object.\n",
    "    \"\"\"\n",
    "    # Extract the text from the BeautifulSoup object and replace newline characters with an empty string\n",
    "    text = soup_obj.text.replace('\\n', \"\")\n",
    "    # Replace tab characters with an empty string\n",
    "    text = text.replace('\\t', \"\")\n",
    "    # Replace carriage return characters with an empty string\n",
    "    text = text.replace('\\r', \"\")\n",
    "\n",
    "    # Return the cleaned text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27086dc2-102b-4bf6-ab8a-08fc3d97464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def editResponse(gptResp):\n",
    "    \"\"\"\n",
    "    Processes the response from the GPT API to extract investment amount and sector information.\n",
    "\n",
    "    Parameters:\n",
    "    gptResp (list): A list of strings representing the chunks of text from the GPT response.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two elements:\n",
    "           - Investment (str): The investment amount extracted from the response.\n",
    "           - Sector (str): The sector extracted from the response.\n",
    "    \"\"\"\n",
    "    # Join all chunks of the GPT response into a single string\n",
    "    respText = ''.join(gptResp)\n",
    "    \n",
    "    # Split the concatenated response text by newline characters to separate lines\n",
    "    InvAmoun_Sector = respText.split('\\n')\n",
    "    \n",
    "    # Extract the investment amount from the first line by splitting on ':' and trimming whitespace\n",
    "    Investment = InvAmoun_Sector[0].split(\":\")[1].strip()\n",
    "    \n",
    "    # Extract the sector from the second line by splitting on ':' and trimming whitespace\n",
    "    Sector = InvAmoun_Sector[1].split(\":\")[1].strip()\n",
    "\n",
    "    # Return the extracted investment amount and sector as a tuple\n",
    "    return Investment, Sector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d8d57b0-c670-4eeb-86e1-5b7153b26794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_InvAmnt_Sector(in_soup):\n",
    "    \"\"\"\n",
    "    Extracts investment amount and sector information from a BeautifulSoup object by\n",
    "    sending the text content to the GPT API and processing the response.\n",
    "\n",
    "    Parameters:\n",
    "    in_soup (BeautifulSoup): The BeautifulSoup object containing the HTML content to be processed.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two elements:\n",
    "           - InvAmnt (str): The extracted investment amount.\n",
    "           - Sector (str): The extracted sector.\n",
    "    \"\"\"\n",
    "    # Clean and extract text from the BeautifulSoup object\n",
    "    text = getCleanTextFromSoup(in_soup)\n",
    "    \n",
    "    # Create a prompt for the GPT API using the cleaned text\n",
    "    gptPrompt = createPrompt(text)\n",
    "    \n",
    "    # Send the prompt to the GPT API and retrieve the response\n",
    "    resp = promptGPT_API(gptPrompt)\n",
    "    \n",
    "    # Process the GPT response to extract investment amount and sector\n",
    "    InvAmnt, Sector = editResponse(resp)\n",
    "\n",
    "    # Return the extracted investment amount and sector as a tuple\n",
    "    return InvAmnt, Sector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e48e739-7d15-476b-91b7-dfc93b0c1f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParnerDataFromInnovateUK(URL, pageNum, target_class_, invList):\n",
    "    \"\"\"\n",
    "    Extracts data about investors from the Innovate UK website and compiles it into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    URL (str): The base URL of the Innovate UK website to start scraping.\n",
    "    pageNum (int): The current page number for pagination.\n",
    "    target_class_ (list): The target class attributes used to filter investor description links.\n",
    "    invList (list): A list to accumulate investor description page links.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the extracted investor data.\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries and lists to store investor data\n",
    "    investorData = {}\n",
    "    investor_nameList = []  # List to store investor names\n",
    "    investor_emailList = []  # List to store investor email addresses\n",
    "    investor_spec_webList = []  # List to store specific web links for each investor\n",
    "    investment_amnt = []  # List to store investment amounts\n",
    "    sector_of_focus = []  # List to store sectors of focus for each investor\n",
    "\n",
    "    # Retrieve all investor description page links by recursively scraping pages\n",
    "    investorDescripLinkList = get_all_investorsLinkDescPagRecursively(URL, pageNum, target_class_, invList)\n",
    "\n",
    "    # Store the list of investor description page links in the dictionary\n",
    "    investorData['Investor_Description_PageLink'] = investorDescripLinkList\n",
    "\n",
    "    num = 1  # Counter for tracking the number of processed investor links\n",
    "    for link in investorDescripLinkList:\n",
    "        print(str(num) + \":\", end=' ')  # Print the current link number\n",
    "        \n",
    "        # Extract and process the HTML content of the investor's description page\n",
    "        tempSoup = web_extract(link)  # Fetch and parse the HTML content using BeautifulSoup\n",
    "        investor_nameList.append(getInvestorName(tempSoup))  # Extract and store investor name\n",
    "        investor_spec_webList.append(getInvestorSpecificPage(tempSoup))  # Extract and store specific web link\n",
    "        investor_emailList.append(getInvestorPartnersEmail(tempSoup))  # Extract and store investor email\n",
    "        inv, sec = get_InvAmnt_Sector(tempSoup)  # Extract investment amount and sector\n",
    "        investment_amnt.append(inv)  # Store the investment amount\n",
    "        sector_of_focus.append(sec)  # Store the sector of focus\n",
    "\n",
    "        num += 1  # Increment the counter\n",
    "\n",
    "    # Store all extracted data in the dictionary\n",
    "    investorData['Investor_Partner_Name'] = investor_nameList\n",
    "    investorData['Investor_Partner_Web'] = investor_spec_webList\n",
    "    investorData['Investor_Partner_EmailContact'] = investor_emailList\n",
    "    investorData['Investment_Amount'] = investment_amnt\n",
    "    investorData['Focused_Sector'] = sector_of_focus\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame (commented out in this code)\n",
    "    # investorDataframe = pd.DataFrame(investorData)\n",
    "\n",
    "    # Return the dictionary containing the extracted investor data\n",
    "    return investorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26239113-b2a1-4d51-8432-03a2e56f15c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: Website successfully retrieved\n",
      "Page 2: Website successfully retrieved\n",
      "Page 3: Website successfully retrieved\n",
      "Page 4: Website successfully retrieved\n",
      "Page 5: Website successfully retrieved\n",
      "Page 6: Website successfully retrieved\n",
      "1: Website successfully retrieved\n",
      "2: Website successfully retrieved\n",
      "3: Website successfully retrieved\n",
      "4: Website successfully retrieved\n",
      "5: Website successfully retrieved\n",
      "6: Website successfully retrieved\n",
      "7: Website successfully retrieved\n",
      "8: Website successfully retrieved\n",
      "9: Website successfully retrieved\n",
      "10: Website successfully retrieved\n",
      "11: Website successfully retrieved\n",
      "12: Website successfully retrieved\n",
      "13: Website successfully retrieved\n",
      "14: Website successfully retrieved\n",
      "15: Website successfully retrieved\n",
      "16: Website successfully retrieved\n",
      "17: Website successfully retrieved\n",
      "18: Website successfully retrieved\n",
      "19: Website successfully retrieved\n",
      "20: Website successfully retrieved\n",
      "21: Website successfully retrieved\n",
      "22: Website successfully retrieved\n",
      "23: Website successfully retrieved\n",
      "24: Website successfully retrieved\n",
      "25: Website successfully retrieved\n",
      "26: Website successfully retrieved\n",
      "27: Website successfully retrieved\n",
      "28: Website successfully retrieved\n",
      "29: Website successfully retrieved\n",
      "30: Website successfully retrieved\n",
      "31: Website successfully retrieved\n",
      "32: Website successfully retrieved\n",
      "33: Website successfully retrieved\n",
      "34: Website successfully retrieved\n",
      "35: Website successfully retrieved\n",
      "36: Website successfully retrieved\n",
      "37: Website successfully retrieved\n",
      "38: Website successfully retrieved\n",
      "39: Website successfully retrieved\n",
      "40: Website successfully retrieved\n",
      "41: Website successfully retrieved\n",
      "42: Website successfully retrieved\n",
      "43: Website successfully retrieved\n",
      "44: Website successfully retrieved\n",
      "45: Website successfully retrieved\n",
      "46: Website successfully retrieved\n",
      "47: Website successfully retrieved\n",
      "48: Website successfully retrieved\n",
      "49: Website successfully retrieved\n",
      "50: Website successfully retrieved\n",
      "51: Website successfully retrieved\n",
      "52: Website successfully retrieved\n",
      "53: Website successfully retrieved\n",
      "54: Website successfully retrieved\n",
      "55: Website successfully retrieved\n",
      "56: Website successfully retrieved\n",
      "57: Website successfully retrieved\n",
      "58: Website successfully retrieved\n",
      "59: Website successfully retrieved\n",
      "60: Website successfully retrieved\n",
      "61: Website successfully retrieved\n",
      "62: Website successfully retrieved\n",
      "63: Website successfully retrieved\n",
      "64: Website successfully retrieved\n",
      "65: Website successfully retrieved\n",
      "66: Website successfully retrieved\n",
      "67: Website successfully retrieved\n",
      "68: Website successfully retrieved\n",
      "69: Website successfully retrieved\n",
      "70: Website successfully retrieved\n",
      "71: Website successfully retrieved\n",
      "72: Website successfully retrieved\n",
      "73: Website successfully retrieved\n",
      "74: Website successfully retrieved\n",
      "75: Website successfully retrieved\n",
      "76: Website successfully retrieved\n",
      "77: Website successfully retrieved\n",
      "78: Website successfully retrieved\n",
      "79: Website successfully retrieved\n",
      "80: Website successfully retrieved\n",
      "81: Website successfully retrieved\n",
      "82: Website successfully retrieved\n",
      "83: Website successfully retrieved\n",
      "84: Website successfully retrieved\n",
      "85: Website successfully retrieved\n",
      "86: Website successfully retrieved\n",
      "87: Website successfully retrieved\n",
      "88: Website successfully retrieved\n",
      "89: Website successfully retrieved\n",
      "90: Website successfully retrieved\n",
      "91: Website successfully retrieved\n",
      "92: Website successfully retrieved\n",
      "93: Website successfully retrieved\n",
      "94: Website successfully retrieved\n",
      "95: Website successfully retrieved\n",
      "96: Website successfully retrieved\n",
      "97: Website successfully retrieved\n",
      "98: Website successfully retrieved\n",
      "99: Website successfully retrieved\n",
      "100: Website successfully retrieved\n",
      "101: Website successfully retrieved\n",
      "102: Website successfully retrieved\n",
      "103: Website successfully retrieved\n",
      "104: Website successfully retrieved\n",
      "105: Website successfully retrieved\n",
      "106: Website successfully retrieved\n",
      "107: Website successfully retrieved\n",
      "108: Website successfully retrieved\n",
      "109: Website successfully retrieved\n",
      "110: Website successfully retrieved\n",
      "111: Website successfully retrieved\n",
      "112: Website successfully retrieved\n",
      "113: Website successfully retrieved\n",
      "114: Website successfully retrieved\n",
      "115: Website successfully retrieved\n",
      "116: Website successfully retrieved\n",
      "117: Website successfully retrieved\n"
     ]
    }
   ],
   "source": [
    "Inv_Data_Dict = getParnerDataFromInnovateUK(baseURL, 1, targetClass, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa4b7a9d-0e59-46ea-95c1-891fc2dddbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "invData_compiled = pd.DataFrame(Inv_Data_Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6777709-da11-468a-ad41-d00c0b354e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Investor_Description_PageLink</th>\n",
       "      <th>Investor_Partner_Name</th>\n",
       "      <th>Investor_Partner_Web</th>\n",
       "      <th>Investor_Partner_EmailContact</th>\n",
       "      <th>Investment_Amount</th>\n",
       "      <th>Focused_Sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>24Haymarket Limited</td>\n",
       "      <td>https://24haymarket.com/</td>\n",
       "      <td>alex@24haymarket.com</td>\n",
       "      <td>'up to £5 million in a round (with a target in...</td>\n",
       "      <td>'cybersecurity, climate tech, supply chain and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>ACT Ventures</td>\n",
       "      <td>https://actvp.vc/</td>\n",
       "      <td>info@actvp.vc</td>\n",
       "      <td>'EUR 500k – EUR 1 Mn as an initial investment....</td>\n",
       "      <td>'Emerging Digital Technologies, Technologies f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>ADA Ventures</td>\n",
       "      <td>https://www.adaventures.com/</td>\n",
       "      <td>enquiries@iuk.ktn-uk.org</td>\n",
       "      <td>'£250,000 – £2m'</td>\n",
       "      <td>'UK technology companies across climate, econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>Albion Capital Group LLP</td>\n",
       "      <td>https://albion.vc/</td>\n",
       "      <td>enquiries@iuk.ktn-uk.org</td>\n",
       "      <td>'None'</td>\n",
       "      <td>'UK early-stage B2B software, deeptech and hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>Amadeus Capital Partners</td>\n",
       "      <td>https://www.amadeuscapital.com/</td>\n",
       "      <td>innovateuk@amadeuscapital.com</td>\n",
       "      <td>'$1 billion'</td>\n",
       "      <td>'Artificial Intelligence (AI), Cybersecurity, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>The Yield Lab</td>\n",
       "      <td>https://theyieldlab.eu/</td>\n",
       "      <td>europe@theyieldlab.com</td>\n",
       "      <td>'None'</td>\n",
       "      <td>'Agtech, AgriFoodTech, crop production, animal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>TSP Ventures</td>\n",
       "      <td>https://tspventures.co.uk/</td>\n",
       "      <td>info@tspventures.co.uk</td>\n",
       "      <td>'None'</td>\n",
       "      <td>'Energy / Decarbonising Heavy Industry; Water ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>Twin Path Ventures</td>\n",
       "      <td>https://www.twinpath.vc</td>\n",
       "      <td>john@twinpath.vc</td>\n",
       "      <td>£500,000</td>\n",
       "      <td>AI-first startups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>Two Magnolias</td>\n",
       "      <td>https://www.twomagnolias.co.uk</td>\n",
       "      <td>hello@twomagnolias.co.uk</td>\n",
       "      <td>'£100k-£500k ticket size'</td>\n",
       "      <td>'Sustainability &amp; Human Health'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>Zinc Ventures</td>\n",
       "      <td>https://www.zinc.vc/</td>\n",
       "      <td>investorpartner@zinc.vc</td>\n",
       "      <td>'None'</td>\n",
       "      <td>'Health of People, Health of Planet'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Investor_Description_PageLink  \\\n",
       "0    https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "1    https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "2    https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "3    https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "4    https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "..                                                 ...   \n",
       "112  https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "113  https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "114  https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "115  https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "116  https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "\n",
       "        Investor_Partner_Name             Investor_Partner_Web  \\\n",
       "0         24Haymarket Limited         https://24haymarket.com/   \n",
       "1                ACT Ventures                https://actvp.vc/   \n",
       "2                ADA Ventures     https://www.adaventures.com/   \n",
       "3    Albion Capital Group LLP               https://albion.vc/   \n",
       "4    Amadeus Capital Partners  https://www.amadeuscapital.com/   \n",
       "..                        ...                              ...   \n",
       "112             The Yield Lab          https://theyieldlab.eu/   \n",
       "113              TSP Ventures       https://tspventures.co.uk/   \n",
       "114        Twin Path Ventures          https://www.twinpath.vc   \n",
       "115             Two Magnolias   https://www.twomagnolias.co.uk   \n",
       "116             Zinc Ventures            https://www.zinc.vc/    \n",
       "\n",
       "     Investor_Partner_EmailContact  \\\n",
       "0             alex@24haymarket.com   \n",
       "1                    info@actvp.vc   \n",
       "2         enquiries@iuk.ktn-uk.org   \n",
       "3         enquiries@iuk.ktn-uk.org   \n",
       "4    innovateuk@amadeuscapital.com   \n",
       "..                             ...   \n",
       "112         europe@theyieldlab.com   \n",
       "113         info@tspventures.co.uk   \n",
       "114               john@twinpath.vc   \n",
       "115       hello@twomagnolias.co.uk   \n",
       "116        investorpartner@zinc.vc   \n",
       "\n",
       "                                     Investment_Amount  \\\n",
       "0    'up to £5 million in a round (with a target in...   \n",
       "1    'EUR 500k – EUR 1 Mn as an initial investment....   \n",
       "2                                     '£250,000 – £2m'   \n",
       "3                                               'None'   \n",
       "4                                         '$1 billion'   \n",
       "..                                                 ...   \n",
       "112                                             'None'   \n",
       "113                                             'None'   \n",
       "114                                           £500,000   \n",
       "115                          '£100k-£500k ticket size'   \n",
       "116                                             'None'   \n",
       "\n",
       "                                        Focused_Sector  \n",
       "0    'cybersecurity, climate tech, supply chain and...  \n",
       "1    'Emerging Digital Technologies, Technologies f...  \n",
       "2    'UK technology companies across climate, econo...  \n",
       "3    'UK early-stage B2B software, deeptech and hea...  \n",
       "4    'Artificial Intelligence (AI), Cybersecurity, ...  \n",
       "..                                                 ...  \n",
       "112  'Agtech, AgriFoodTech, crop production, animal...  \n",
       "113  'Energy / Decarbonising Heavy Industry; Water ...  \n",
       "114                                  AI-first startups  \n",
       "115                    'Sustainability & Human Health'  \n",
       "116               'Health of People, Health of Planet'  \n",
       "\n",
       "[117 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invData_compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3935a5a-1a57-4ec1-b6bc-b741a4f9fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "invData_compiled.to_csv('compiledData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e870b37-189d-49bf-944e-b946b264abbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

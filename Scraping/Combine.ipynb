{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b02fab-ce96-4519-8ed8-8eddbc6a40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the 'requests' library to handle HTTP requests\n",
    "import requests\n",
    "\n",
    "# Importing 'BeautifulSoup' from the 'bs4' library to parse HTML and XML documents\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c069bba-e509-4231-b404-7c1226abf9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the BeautifulSoup object which contains the web structure and content of a URL\n",
    "def web_extract(url):\n",
    "    # Declare variable soup and initialize it to None\n",
    "    soup = None\n",
    "    try:\n",
    "        # Make an HTTP request to the given URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the HTTP request is successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the response using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            print(\"Website successfully retrieved\")\n",
    "        # If there is any issue with the HTTP request, print the status code\n",
    "        else:\n",
    "            print(f\"Fail to retrieve the web page. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that occur during the HTTP request\n",
    "        print(f\"Error processing the link {url}: {e}\")\n",
    "    \n",
    "    # Return the BeautifulSoup object\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e8ebfa-3df1-46b5-b1c7-10476d95667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the URL of the base page of \n",
    "#->Innovate UK's Investor Partnerships Future Economy \n",
    "#-->projects to the variable baseURL\n",
    "baseURL = 'https://iuk.ktn-uk.org/projects/?_sft_programme=investor-partnerships-future-economy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a7372d-f54f-45f7-b100-b49f3cf6ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class attribute to help extract each of the investors description page link\n",
    "targetClass =  ['group', 'border-2', 'border-primary', 'flex', 'flex-col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc91f48-6658-4ee9-aced-1daf0ad12baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDescriptionPageLink(soup, targetClass):\n",
    "    \"\"\"\n",
    "    Extracts links to description pages from a BeautifulSoup object.\n",
    "    \n",
    "    Parameters:\n",
    "    soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML of the webpage.\n",
    "    targetClass (list): The class attribute to filter the target <a> tags.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of links to the description pages.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all <a> tags with the specified class attribute\n",
    "    target_a_tag = [tag for tag in soup.find_all('a')\n",
    "                    if 'class' in tag.attrs\n",
    "                    and tag.attrs['class'] == targetClass]\n",
    "\n",
    "    # Extract the href attribute (link) from each <a> tag\n",
    "    hrefList = [tag.get('href') for tag in target_a_tag]\n",
    "\n",
    "    return hrefList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1910a705-e144-4bc8-bfdf-478c985a9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextPage(soup):\n",
    "    \"\"\"\n",
    "    Retrieves the URL of the next page from the BeautifulSoup object.\n",
    "\n",
    "    Parameters:\n",
    "    soup (BeautifulSoup): BeautifulSoup object representing the current page.\n",
    "\n",
    "    Returns:\n",
    "    str or None: URL of the next page if found, otherwise None.\n",
    "    \"\"\"\n",
    "    next_atag = soup.find(lambda tag: tag.name == 'a' and tag.text == 'Next >')\n",
    "    \n",
    "    if next_atag:\n",
    "        return next_atag.get('href')\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56b690fc-b06c-4068-9332-0614eabae8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_investorsLinkDescPagRecursively(URL, pageNum, target_class_, invList):\n",
    "    \"\"\"\n",
    "    Recursively navigates through pages to collect links to investor description pages.\n",
    "    \n",
    "    Parameters:\n",
    "    URL (str): The URL of the current page.\n",
    "    pageNum (int): The current page number.\n",
    "    target_class_ (list): The class attribute to filter the target <a> tags.\n",
    "    invList (list): The list to store the collected links.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of all collected links to investor description pages from all pages.\n",
    "    \"\"\"\n",
    "    # Create a copy of the current list of links\n",
    "    currentList = invList.copy()\n",
    "    \n",
    "    # Print the current page number\n",
    "    print(f\"Page {pageNum}: \", end='')\n",
    "    \n",
    "    # Extract the BeautifulSoup object from the URL\n",
    "    soup = web_extract(URL)\n",
    "    \n",
    "    # Extend the current list with links extracted from the current page\n",
    "    currentList.extend(getDescriptionPageLink(soup, target_class_))\n",
    "    \n",
    "    # Get the URL for the next page\n",
    "    nextURL = getNextPage(soup)\n",
    "    \n",
    "    # If there is a next page, increment the page number and continue recursion\n",
    "    if nextURL:\n",
    "        pageNum += 1\n",
    "        return get_all_investorsLinkDescPagRecursively(nextURL, pageNum, target_class_, currentList)\n",
    "    else:\n",
    "        # Return the compiled list of links when no more pages are available\n",
    "        return currentList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f28f5279-f6a2-4eea-b5ab-51ad335a56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInvestorName(in_soup):\n",
    "    \"\"\"\n",
    "    Extracts the investor's name from the title of the investor's description page.\n",
    "\n",
    "    Parameters:\n",
    "    in_soup (BeautifulSoup object): The BeautifulSoup object representing the investor's \n",
    "    description page.\n",
    "\n",
    "    Returns:\n",
    "    str: The extracted investor's name.\n",
    "    \"\"\"\n",
    "    # Extract the text from the <title> tag of the investor's description page\n",
    "    name = in_soup.title.text\n",
    "    \n",
    "    # Split the title text by the '-' character to separate the name from any additional details\n",
    "    splitName = name.split('-')\n",
    "    \n",
    "    # Take the first part of the split text as the investor's name and remove any leading/trailing whitespace\n",
    "    name = splitName[0].strip()\n",
    "    \n",
    "    # Return the cleaned investor's name\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "529d22e6-9e6c-4277-85ee-579ad1b22a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInvestorSpecificPage(in_soup):\n",
    "    \"\"\"\n",
    "    Extracts the URL of the specific investor's page from the BeautifulSoup object of the \n",
    "    investor's description page.\n",
    "\n",
    "    Parameters:\n",
    "    in_soup (BeautifulSoup object): The BeautifulSoup object representing the investor's \n",
    "    description page.\n",
    "\n",
    "    Returns:\n",
    "    str or None: The URL of the specific investor's page if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # Find the tag with the specified class attributes indicating the link to the investor's page\n",
    "    targetTag = in_soup.find(lambda tag: 'class' in tag.attrs and \n",
    "                          tag.attrs['class'] == ['line-clamp-1', 'underline'])\n",
    "    \n",
    "    # If the target tag is found, extract the href attribute from the <a> tag within it\n",
    "    if targetTag:\n",
    "       return targetTag.find('a').get('href')\n",
    "    else:\n",
    "        # Return None if the target tag is not found\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1159ea-5ec4-4af2-b817-f0044620fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInvestorPartnersEmail(in_soup):\n",
    "    \"\"\"\n",
    "    Extracts the email address of the investor's partner from the BeautifulSoup \n",
    "    object of the investor's description page.\n",
    "\n",
    "    Parameters:\n",
    "    in_soup (BeautifulSoup object): The BeautifulSoup object representing the \n",
    "    investor's description page.\n",
    "\n",
    "    Returns:\n",
    "    str or None: The email address of the investor's partner if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # Find the <a> tag with an 'href' attribute starting with 'mailto:'\n",
    "    tempTag = in_soup.find('a', href=lambda href: href and href.startswith('mailto:'))\n",
    "    \n",
    "    # If the <a> tag is found, extract and return the email address\n",
    "    if tempTag:\n",
    "        mailTo = tempTag.get('href')  # Get the 'href' attribute value\n",
    "        splitMailText = mailTo.split(':')  # Split the string at ':'\n",
    "        return splitMailText[1]  # Return the part after ':'\n",
    "    else:\n",
    "        # Return None if no email address is found\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fde45ae9-9875-4030-bd5c-10e03b118bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def getParnerDataFromInnovateUK(URL, pageNum, target_class_, invList):\n",
    "    \"\"\"\n",
    "    Extracts data about investors from the Innovate UK website and compiles it into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    URL (str): The base URL of the Innovate UK website to start scraping.\n",
    "    pageNum (int): The current page number for pagination.\n",
    "    target_class_ (list): The target class attributes used to filter investor description links.\n",
    "    invList (list): A list to accumulate investor description page links.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the extracted investor data.\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries and lists to store investor data\n",
    "    investorData = {}\n",
    "    investor_nameList = []\n",
    "    investor_emailList = []\n",
    "    investor_spec_webList = []\n",
    "\n",
    "    # Retrieve all investor description page links\n",
    "    investorDescripLinkList = get_all_investorsLinkDescPagRecursively(URL, pageNum, target_class_, invList)\n",
    "\n",
    "    # Store the list of investor description page links in the dictionary\n",
    "    investorData['Investor_Description_PageLink'] = investorDescripLinkList\n",
    "\n",
    "    num = 1  # Counter for tracking the number of processed links\n",
    "    for link in investorDescripLinkList:\n",
    "        print(str(num) + \":\", end=' ')\n",
    "        \n",
    "        # Extract and process the HTML content of the investor's description page\n",
    "        tempSoup = web_extract(link)\n",
    "        investor_nameList.append(getInvestorName(tempSoup))  # Extract investor name\n",
    "        investor_spec_webList.append(getInvestorSpecificPage(tempSoup))  # Extract specific web link\n",
    "        investor_emailList.append(getInvestorPartnersEmail(tempSoup))  # Extract investor email\n",
    "\n",
    "        num += 1  # Increment the counter\n",
    "\n",
    "    # Store the extracted data in the dictionary\n",
    "    investorData['Investor_Partner_Name'] = investor_nameList\n",
    "    investorData['Investor_Partner_Web'] = investor_spec_webList\n",
    "    investorData['Investor_Partner_EmailContact'] = investor_emailList\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame\n",
    "    investorDataframe = pd.DataFrame(investorData)\n",
    "\n",
    "    return investorDataframe  # Return the DataFrame containing the investor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33a6573c-5115-4416-97a3-94245820c1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: Website successfully retrieved\n",
      "Page 2: Website successfully retrieved\n",
      "Page 3: Website successfully retrieved\n",
      "Page 4: Website successfully retrieved\n",
      "Page 5: Website successfully retrieved\n",
      "Page 6: Website successfully retrieved\n",
      "1: Website successfully retrieved\n",
      "2: Website successfully retrieved\n",
      "3: Website successfully retrieved\n",
      "4: Website successfully retrieved\n",
      "5: Website successfully retrieved\n",
      "6: Website successfully retrieved\n",
      "7: Website successfully retrieved\n",
      "8: Website successfully retrieved\n",
      "9: Website successfully retrieved\n",
      "10: Website successfully retrieved\n",
      "11: Website successfully retrieved\n",
      "12: Website successfully retrieved\n",
      "13: Website successfully retrieved\n",
      "14: Website successfully retrieved\n",
      "15: Website successfully retrieved\n",
      "16: Website successfully retrieved\n",
      "17: Website successfully retrieved\n",
      "18: Website successfully retrieved\n",
      "19: Website successfully retrieved\n",
      "20: Website successfully retrieved\n",
      "21: Website successfully retrieved\n",
      "22: Website successfully retrieved\n",
      "23: Website successfully retrieved\n",
      "24: Website successfully retrieved\n",
      "25: Website successfully retrieved\n",
      "26: Website successfully retrieved\n",
      "27: Website successfully retrieved\n",
      "28: Website successfully retrieved\n",
      "29: Website successfully retrieved\n",
      "30: Website successfully retrieved\n",
      "31: Website successfully retrieved\n",
      "32: Website successfully retrieved\n",
      "33: Website successfully retrieved\n",
      "34: Website successfully retrieved\n",
      "35: Website successfully retrieved\n",
      "36: Website successfully retrieved\n",
      "37: Website successfully retrieved\n",
      "38: Website successfully retrieved\n",
      "39: Website successfully retrieved\n",
      "40: Website successfully retrieved\n",
      "41: Website successfully retrieved\n",
      "42: Website successfully retrieved\n",
      "43: Website successfully retrieved\n",
      "44: Website successfully retrieved\n",
      "45: Website successfully retrieved\n",
      "46: Website successfully retrieved\n",
      "47: Website successfully retrieved\n",
      "48: Website successfully retrieved\n",
      "49: Website successfully retrieved\n",
      "50: Website successfully retrieved\n",
      "51: Website successfully retrieved\n",
      "52: Website successfully retrieved\n",
      "53: Website successfully retrieved\n",
      "54: Website successfully retrieved\n",
      "55: Website successfully retrieved\n",
      "56: Website successfully retrieved\n",
      "57: Website successfully retrieved\n",
      "58: Website successfully retrieved\n",
      "59: Website successfully retrieved\n",
      "60: Website successfully retrieved\n",
      "61: Website successfully retrieved\n",
      "62: Website successfully retrieved\n",
      "63: Website successfully retrieved\n",
      "64: Website successfully retrieved\n",
      "65: Website successfully retrieved\n",
      "66: Website successfully retrieved\n",
      "67: Website successfully retrieved\n",
      "68: Website successfully retrieved\n",
      "69: Website successfully retrieved\n",
      "70: Website successfully retrieved\n",
      "71: Website successfully retrieved\n",
      "72: Website successfully retrieved\n",
      "73: Website successfully retrieved\n",
      "74: Website successfully retrieved\n",
      "75: Website successfully retrieved\n",
      "76: Website successfully retrieved\n",
      "77: Website successfully retrieved\n",
      "78: Website successfully retrieved\n",
      "79: Website successfully retrieved\n",
      "80: Website successfully retrieved\n",
      "81: Website successfully retrieved\n",
      "82: Website successfully retrieved\n",
      "83: Website successfully retrieved\n",
      "84: Website successfully retrieved\n",
      "85: Website successfully retrieved\n",
      "86: Website successfully retrieved\n",
      "87: Website successfully retrieved\n",
      "88: Website successfully retrieved\n",
      "89: Website successfully retrieved\n",
      "90: Website successfully retrieved\n",
      "91: Website successfully retrieved\n",
      "92: Website successfully retrieved\n",
      "93: Website successfully retrieved\n",
      "94: Website successfully retrieved\n",
      "95: Website successfully retrieved\n",
      "96: Website successfully retrieved\n",
      "97: Website successfully retrieved\n",
      "98: Website successfully retrieved\n",
      "99: Website successfully retrieved\n",
      "100: Website successfully retrieved\n",
      "101: Website successfully retrieved\n",
      "102: Website successfully retrieved\n",
      "103: Website successfully retrieved\n",
      "104: Website successfully retrieved\n",
      "105: Website successfully retrieved\n",
      "106: Website successfully retrieved\n",
      "107: Website successfully retrieved\n",
      "108: Website successfully retrieved\n",
      "109: Website successfully retrieved\n",
      "110: Website successfully retrieved\n",
      "111: Website successfully retrieved\n",
      "112: Website successfully retrieved\n",
      "113: Website successfully retrieved\n",
      "114: Website successfully retrieved\n",
      "115: Website successfully retrieved\n",
      "116: Website successfully retrieved\n",
      "117: Website successfully retrieved\n"
     ]
    }
   ],
   "source": [
    "investorData = getParnerDataFromInnovateUK(baseURL, 1, targetClass, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e18a595-23e4-40fd-b556-a1d067646c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Investor_Description_PageLink</th>\n",
       "      <th>Investor_Partner_Name</th>\n",
       "      <th>Investor_Partner_Web</th>\n",
       "      <th>Investor_Partner_EmailContact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>24Haymarket Limited</td>\n",
       "      <td>https://24haymarket.com/</td>\n",
       "      <td>alex@24haymarket.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>ACT Ventures</td>\n",
       "      <td>https://actvp.vc/</td>\n",
       "      <td>info@actvp.vc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>ADA Ventures</td>\n",
       "      <td>https://www.adaventures.com/</td>\n",
       "      <td>enquiries@iuk.ktn-uk.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>Albion Capital Group LLP</td>\n",
       "      <td>https://albion.vc/</td>\n",
       "      <td>enquiries@iuk.ktn-uk.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>Amadeus Capital Partners</td>\n",
       "      <td>https://www.amadeuscapital.com/</td>\n",
       "      <td>innovateuk@amadeuscapital.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>The Yield Lab</td>\n",
       "      <td>https://theyieldlab.eu/</td>\n",
       "      <td>europe@theyieldlab.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>TSP Ventures</td>\n",
       "      <td>https://tspventures.co.uk/</td>\n",
       "      <td>info@tspventures.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>Twin Path Ventures</td>\n",
       "      <td>https://www.twinpath.vc</td>\n",
       "      <td>john@twinpath.vc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>Two Magnolias</td>\n",
       "      <td>https://www.twomagnolias.co.uk</td>\n",
       "      <td>hello@twomagnolias.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>https://iuk.ktn-uk.org/projects/investor-partn...</td>\n",
       "      <td>Zinc Ventures</td>\n",
       "      <td>https://www.zinc.vc/</td>\n",
       "      <td>investorpartner@zinc.vc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Investor_Description_PageLink  \\\n",
       "0    https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "1    https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "2    https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "3    https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "4    https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "..                                                 ...   \n",
       "112  https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "113  https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "114  https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "115  https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "116  https://iuk.ktn-uk.org/projects/investor-partn...   \n",
       "\n",
       "        Investor_Partner_Name             Investor_Partner_Web  \\\n",
       "0         24Haymarket Limited         https://24haymarket.com/   \n",
       "1                ACT Ventures                https://actvp.vc/   \n",
       "2                ADA Ventures     https://www.adaventures.com/   \n",
       "3    Albion Capital Group LLP               https://albion.vc/   \n",
       "4    Amadeus Capital Partners  https://www.amadeuscapital.com/   \n",
       "..                        ...                              ...   \n",
       "112             The Yield Lab          https://theyieldlab.eu/   \n",
       "113              TSP Ventures       https://tspventures.co.uk/   \n",
       "114        Twin Path Ventures          https://www.twinpath.vc   \n",
       "115             Two Magnolias   https://www.twomagnolias.co.uk   \n",
       "116             Zinc Ventures            https://www.zinc.vc/    \n",
       "\n",
       "     Investor_Partner_EmailContact  \n",
       "0             alex@24haymarket.com  \n",
       "1                    info@actvp.vc  \n",
       "2         enquiries@iuk.ktn-uk.org  \n",
       "3         enquiries@iuk.ktn-uk.org  \n",
       "4    innovateuk@amadeuscapital.com  \n",
       "..                             ...  \n",
       "112         europe@theyieldlab.com  \n",
       "113         info@tspventures.co.uk  \n",
       "114               john@twinpath.vc  \n",
       "115       hello@twomagnolias.co.uk  \n",
       "116        investorpartner@zinc.vc  \n",
       "\n",
       "[117 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c7e4a-9a51-4db1-9b61-c8ccc29dde13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
